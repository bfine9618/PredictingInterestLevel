# ESE 305 FinalProject
### Braden Fineberg, Brooke Behrbaum, Brianna Gallo

The initial dataset can be found in ./rawData. I have also used the script 'clean data' to standardize columns, rows and names to use for the rest of the analysis. Finally, using the NLP script, i was successful in determining the type of rental from 75% of the data. After running a basic OLS regression with typed data, we are able to predict with higher accuracy what the interest level in each home is. 

The file Brooke_Interaction_Terms explores adding interaction terms to the data and running linear regression to improve the model fit.  Interaction terms were added to combine specific related predictors, such as allowing pets in general by synthesizing allowing cats and allowing dogs.  By creating interaction terms, we hope to find an interaction term which allows us get stratification across interest level.  One interaction term we added, outdoor_score, achieved some stratification.  This term calculates an average score for the number of outdoor features a listing has.  We discovered that only high interest listings have all of these features.

The file Tree explores various tree models on the original dataset, starting at a standard DecisionTreeClassifier(), and then performing hyperparameter optimization, using K-Fold to perform cross validation on the tree at different depths. The results were analyzed and from there we moved on to bagging. The bagged tree performed better than the Decision Tree by almost 2%. In an effort to find a tree model that, on all the predictors, unchanged, would perform a better classification, we then moved on to Random Forests. The hyperparameters were chosen again by K-Fold cross validation, however some difficulties were reached on having sufficient computational power to run the cross validation. After using warm_start, the number of optimal trees was determined to be roughly 170, although the cross validation results showed slightly better accuracies for larger numbers (>300). Moving on to the max number of features to be used, another K-Fold cross validation was run, however warm_start no longer benefitted us, so the best results we were able to obtain are from running each tree only once and testing its accuracy (without using K-Fold). The results are unsatisfactory, but moving forward we will be attempting tree-based classification with better training data that will ideally speed up the hyper-parameter selection process. After the above tree based clasification methods were simulated, we briefly studied the results of the most optimal (at this point in time) RandomForestClassifier, including the predicted probabilities for each class for each sample. Moving forward, we are going to study how potentially changing the threshhold for classification may change our acccuracy. As it stands, the tree based models have a very high accuracy for "low" interest houses at the expense of medium and high ones.


