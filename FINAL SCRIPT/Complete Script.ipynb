{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV as logCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Visualization:\n",
    "Visualizations can be found within most of our python files. Visualizing the data helped us understand how different models and interaction terms preformed. However, our initial visualization can be found [here](https://github.com/bfine9618/ese305FinalProject/blob/master/figures/Initial%20Visualization.ipynb).\n",
    "\n",
    "# Initial Exploration of Various Models:\n",
    "   * [Trees (Standard Tree, Random Forest Classifier, Bagged Trees)](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/Trees.ipynb)\n",
    "   * [Linear Model (to get a sense of relevant predictors)](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/linear_model.ipynb)\n",
    "    \n",
    "    \n",
    "# Exploration of Features/Data Cleaning:\n",
    "\n",
    "   * [Testing for Significant Interaction Terms](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/exploring_interaction_terms.ipynb)\n",
    "   * [Using NLP to classify housing listings by the type of home](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/NLP.ipynb)\n",
    "   * [Data Cleaning (describe more)](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/Data%20Cleaning%20and%20Pre-Processing.ipynb)\n",
    "   * [Data Cleaning 2 (describe more)](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/Merge%20Data%20Files.ipynb)\n",
    "   \n",
    "# Model Exploration\n",
    "* [Model 1](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/Trees_Typed_Data.ipynb) : Fitting a Random Forest Classifier on the 'typed' housing listings (see NLP) and exploring fitting a series of 2-class Random Forest Classifiers to improve prediction:\n",
    "* Model 2: SVMs\n",
    "    * [SVM Initial Exploration](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/SVM_Exploration.ipynb)\n",
    "    * [Hyperparameter Selection (of gamma and c)](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/hyperparameter_selection_SVM.ipynb)\n",
    "* [Model 3](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/NeuralNets.ipynb): Nural Nets with subpar success\n",
    "* [Model 4](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/KNN%20%26%20CV%20Logistic%20Reg.ipynb): KNN and logistic regression on cleaned, typed, and pre-processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model:\n",
    "\n",
    "We broke our final model into two complete functions. One cleans and pre-processes the data (cleanPreprocessData()), while the other actually runs our learning model. To clean the data, we took the most successful aspects of each individual model to create terms that begin to separate interest levels. For example, in New York, as expected, a balcony and access to a common outdoor space are highly valued; therefore, we created an interaction term \"outdoor_score\" that attempts to capture how \"outdoor-sy\" the building is. We began to generate these terms for a wide range of rental units and combinations. In addition, we used basic NLP to classify the units. This is used to create market expectations for feature combinations, price points, and more importantly price/feature. Using this, we are able to tell if a specific unit is listed above or below predicted market value for that specific type of home. For example, what renters look for in a studio is very different than the market for penthouses. Some of these interaction terms are \"price_per_feature\", \"price_per_num_lux\", and \"price_feature_ratio\" among others. However, the most important interaction term is the market immediately surrounding that specific rental. We determined this through computing an expected interest level for every building and unit manager. The initial exploration of these terms can be found [here](https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/KNN%20%26%20CV%20Logistic%20Reg.ipynb) in cell 6. This specific separation allowed us to use a multi-class logrithmic classifier to determine the interest level of units. \n",
    "\n",
    "Ultimately, our model is two parts: a logrithmic classifier for the best data and a SVM for any data that cannot be classified well with the log model. We determine data quality by whether we know a units expected interest level, \"prob_buildManager\". If we have this information, we are able to classify the interst_level of a unit with 92% accuracy. Wihtout it, our accuracy falls to only 71%, not much better than guessing. Therefore, given this dataset, our expected accuracy is approximatly 81%. In future models, there may be a way to predict \"prob_buildManager\" by running a KNN on the lat long data. If we can do this with high accuracy we can improve our overall model. Essentially, we found that the mantra of real-estate agents is valid. When buying or renting a home, **'LOCATION, LOCATION, LOCATION.'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are our helper functions and classes. \n",
    "# They each assist with a different part \n",
    "# of cleaning and preprocessing the data.\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def descrClean(x):\n",
    "    des = strip_tags(x)\n",
    "    return des.lower()\n",
    "\n",
    "# Function to Classify Unit Types\n",
    "def unitType(x, types):\n",
    "    homeType = {    \n",
    "        }\n",
    "    for lst in types:\n",
    "        homeType[lst[0]] = False\n",
    "    \n",
    "    for lst in types:\n",
    "        for w in lst:\n",
    "            if w in x:\n",
    "                return lst[0]\n",
    "    return 'other'\n",
    "\n",
    "#Dealing with lofts or studios that have no rooms \n",
    "def pricePerRoom(row):\n",
    "    if row['rooms']==0:\n",
    "        return row['price']/.5\n",
    "    else:\n",
    "        return row['price']/row['rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanPreprocessData(train, test):\n",
    "    print('Cleaning...')\n",
    "    df = train\n",
    "    test['test'] = True\n",
    "\n",
    "    #Merge the two files to clean and comput interaction terms.\n",
    "    df = df.append(test)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['test'].fillna(False, inplace=True)\n",
    "\n",
    "    #Clean the column names for regressions and ML Models\n",
    "    df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('-', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('/', '_') for c in df.columns]\n",
    "\n",
    "    #Confirm types for columns with numbers\n",
    "    df['bedrooms'] = df['bedrooms'].apply(float)\n",
    "    df['bedrooms'].fillna(0, inplace=True)\n",
    "    df['bathrooms'].fillna(0, inplace=True)\n",
    "\n",
    "    #Drop meaningless columns in data\n",
    "    df.drop(['index', 'level_0'], axis=1, inplace=True)\n",
    "\n",
    "    #Map Interest levels to values for OLS Regression\n",
    "    df['interestVal'] = df['interest_level'].map({'high': 1, \n",
    "                                                  'medium': 0.5, \n",
    "                                                  'low':0})\n",
    "\n",
    "    #Clean the HTML from descriptions to allow for NLP\n",
    "    df['description'] = df['description'].apply(descrClean)\n",
    "\n",
    "    # Aggregate to create one laundry in building \n",
    "    # column that isn't case sensitive\n",
    "    df['laundry_in_building'] = df.apply(lambda row: \n",
    "                                         row['Laundry_in_Building'] or \n",
    "                                         row['Laundry_In_Building'], \n",
    "                                         axis=1)\n",
    "\n",
    "    # Drop old laundry in building columns\n",
    "    df = df.drop(['Laundry_in_Building', 'Laundry_In_Building'], axis=1)\n",
    "\n",
    "    cleanedDf = df\n",
    "\n",
    "    print('Cleaning Complete. ' +\n",
    "          'Processing descriptions to determine type...')\n",
    "\n",
    "    # To determine the type of rental unit, we conduct a basic NLP\n",
    "    # Define basic unit types\n",
    "    apt = ['apartment', 'apt']\n",
    "    condo = ['condominium', 'condo']\n",
    "    walkUp = ['walk_up', 'walk-up', 'walkup', 'walk up']\n",
    "    studio = ['studio']\n",
    "    ph = ['ph', 'penhouse']\n",
    "    townhome = ['townhome', 'duplex', 'townhouse']\n",
    "    loft = ['loft']\n",
    "\n",
    "    types = [apt, condo, walkUp, studio, ph, townhome, loft]\n",
    "\n",
    "    #Determine rental type\n",
    "    df['type'] = df['description'].apply(lambda x : unitType(x, types)) \n",
    "\n",
    "    #Determine if a type has been found\n",
    "    df['foundType'] = ~df['type'].str.contains('other') \n",
    "\n",
    "    #Create binary dummy columns for each type\n",
    "    df = pd.concat([df, pd.get_dummies(df['type'])], axis=1) \n",
    "\n",
    "    #Combine and drop the two loft column\n",
    "    df['loft'].fillna(False, inplace=True)\n",
    "    df['loft'] = df[['loft', 'Loft']].apply(lambda row : \n",
    "                                            row['loft'] or \n",
    "                                            row['Loft'], \n",
    "                                            axis=1)\n",
    "    df.drop('Loft', axis=1, inplace=True)\n",
    "\n",
    "    cleanedTyped = df\n",
    "\n",
    "    print('Typing Complete. Generating Interaction Terms...')\n",
    "\n",
    "    # Generate interaction terms to find differentiators\n",
    "    # Luxury Score Term - higher the score means the more \n",
    "    # luxury items included\n",
    "    df['lux_score'] = (df['Exclusive'] + df['Doorman'] + \n",
    "                       df['Outdoor_Space'] + df['New_Construction'] + \n",
    "                       df['Roof_Deck'] + df['Fitness_Center'] +\n",
    "                       df['Swimming_Pool'] + df['Elevator'] + \n",
    "                       df['Laundry_in_Unit'] + \n",
    "                       df['Hardwood_Floors']) / 10\n",
    "\n",
    "    # Group data by buildings and agents to determine expected interest \n",
    "    # -----MAGIC FEATURE----- \n",
    "    agentGroup = df.groupby(['manager_id']).mean()\n",
    "    buildingGroup = df.groupby(['building_id', 'manager_id']).mean()\n",
    "\n",
    "    buildingAvg = buildingGroup[['interestVal']]\n",
    "    buildingAvg.columns = ['prob_interest_building']\n",
    "    buildingAvg.reset_index(inplace=True)\n",
    "\n",
    "    managerAvg = agentGroup[['interestVal']]\n",
    "    managerAvg.columns = ['prob_interest_manager']\n",
    "    managerAvg.reset_index(inplace=True)\n",
    "\n",
    "    #Merge back to original DF\n",
    "    df = df.merge(managerAvg, on='manager_id', how='left')\n",
    "    df = df.merge(buildingAvg, on=['building_id', 'manager_id'], \n",
    "                  how='left')\n",
    "\n",
    "    #Compute expected interest by building and manager\n",
    "    df['prob_buildManager'] = (df['prob_interest_building']+\n",
    "                               df['prob_interest_manager'])/2\n",
    "\n",
    "    #Count rooms and determine price per room\n",
    "    df['rooms'] = df['bedrooms']+df['bathrooms']\n",
    "    \n",
    "    df['price_per_room'] = df[['price', 'rooms']].apply(pricePerRoom, \n",
    "                                                        axis=1)\n",
    "\n",
    "    # Number of Luxury Features Term\n",
    "    df['num_luxury'] = (df['Exclusive'] + df['Doorman'] + \n",
    "                        df['Outdoor_Space'] + df['New_Construction'] + \n",
    "                        df['Roof_Deck'] + df['Fitness_Center'] + \n",
    "                        df['Swimming_Pool'] + df['Elevator'] + \n",
    "                        df['Laundry_in_Unit'] + df['Hardwood_Floors'])\n",
    "\n",
    "    # Number of Features per Listing\n",
    "    df['num_features'] = df['features'].apply(len)\n",
    "\n",
    "    # ADA compatible interaction term\n",
    "    # 1 if both elevator and wheelchair access, 0 if one or \n",
    "    # neither are included\n",
    "    df['ada'] = df['Elevator'] * df['Wheelchair_Access']\n",
    "\n",
    "    # Create transformed term that creates a score for outdoor spaces\n",
    "    # Higher the score, the more of these features are included\n",
    "    df['outdoor_score'] = (df['Outdoor_Space'] + df['Balcony'] + \n",
    "                           df['Common_Outdoor_Space'] + \n",
    "                           df['Garden_Patio'] + \n",
    "                           df['Roof_Deck'] + df['Terrace']) / 6\n",
    "\n",
    "    # Create interaction term for fitness oriented\n",
    "    # 1 if both swimming pool and fitness center are included, \n",
    "    # 0 if one or neither included\n",
    "    df['fitness_oriented'] = df['Fitness_Center'] * df['Swimming_Pool']\n",
    "\n",
    "    # Create interaction term for doorman/exclusive\n",
    "    # 1 if both are included, 0 if one or neither are included\n",
    "    df['door_excl'] = df['Doorman'] * df['Exclusive']\n",
    "\n",
    "    # Create interaction term for cats and dogs allowed\n",
    "    # 1 if both are allowed, 0 if one or neither are allowed\n",
    "    df['pets_allowed'] = df['Cats_Allowed'] * df['Dogs_Allowed']\n",
    "\n",
    "    #Compute price per feature and price per luxury feature. \n",
    "    #If no features exist, the value is empty\n",
    "    df['price_per_feature'] = df['price']/df['num_features']\n",
    "    df['price_per_feature'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    df['price_per_num_lux'] = df['price']/df['num_luxury']\n",
    "    df['price_per_num_lux'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    #Determine expected prices by type of unit\n",
    "    g1 = df.groupby(['type']).mean()\n",
    "    g1.reset_index(inplace=True)\n",
    "\n",
    "    #Columns we wish to average\n",
    "    avgs = g1[['type','lux_score', 'num_features', \n",
    "               'num_luxury','outdoor_score', 'price_per_num_lux', \n",
    "               'price_per_feature']]\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    #Rename columns and merge back to original DF\n",
    "    avgs.columns = ['avg_'+x for x in avgs]\n",
    "    avgs.rename(columns={'avg_type':'type'}, inplace=True)\n",
    "    df = pd.merge(df, avgs, on='type')\n",
    "\n",
    "    #If no price was found, set the price for the column as average \n",
    "    # to avoid skewing the data\n",
    "    df['price_per_num_lux'].fillna(df['avg_price_per_num_lux'], \n",
    "                                   inplace=True)\n",
    "    df['outdoor_score'].fillna(df['avg_outdoor_score'], inplace=True)\n",
    "    df['lux_score'].fillna(df['avg_lux_score'], inplace=True)\n",
    "    df['price_per_feature'].fillna(df['avg_price_per_feature'], \n",
    "                                   inplace=True)\n",
    "\n",
    "    df['price_lux_ratio'] = (df['price_per_num_lux']/\n",
    "                             df['avg_price_per_num_lux'])\n",
    "    df['outdoor_ratio'] = df['outdoor_score']/df['avg_outdoor_score']\n",
    "    df['lux_ratio'] = df['lux_score']/df['avg_lux_score']\n",
    "    df['price_feature_ratio'] = (df['price_per_feature']/\n",
    "                                 df['avg_price_per_feature'])\n",
    "\n",
    "\n",
    "    #Compute the number of photos included in the listing\n",
    "    df['numPhotos'] = df['photos'].apply(len)\n",
    "\n",
    "    #Listing id is an arbitrary int label assined to each listing. \n",
    "    # not useful for classification\n",
    "    df.drop(['listing_id'], axis=1, inplace=True)\n",
    "\n",
    "    #Output new training and testing datasets\n",
    "    train = pd.DataFrame(df[df['test']==False].dropna())\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    train.drop('test', inplace=True, axis=1)\n",
    "    \n",
    "    test = pd.DataFrame(df[df['test']])\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    test.drop('test', inplace=True, axis=1)\n",
    "\n",
    "    train.to_json('./cleaned/train.json')\n",
    "    test.to_json('./cleaned/test.json')\n",
    "    print('Cleaning and Preprocessing complete.')\n",
    "    return cleanedDf, cleanedTyped, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModel():\n",
    "    \n",
    "    train = pd.read_json('./cleaned/train.json')\n",
    "    test = pd.read_json('./cleaned/test.json')\n",
    "    \n",
    "    #Determine the columns with which to run an OLS, exclude the \n",
    "    # indicator column\n",
    "    data = train.drop('interestVal', \n",
    "                      axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    #join columns to build to equation\n",
    "    equation = ('+').join(data.columns)\n",
    "\n",
    "    #run the OLS to determine significant columns\n",
    "    model = smf.ols('interestVal~'+equation, data=train).fit()\n",
    "\n",
    "    #make a DF of significant features\n",
    "    sig_features = pd.DataFrame(model.pvalues, index=data.columns, \n",
    "                                columns={'P_Value'})\n",
    "\n",
    "    sigCols = sig_features[sig_features['P_Value']<.1].index.values\n",
    "    print('The data has {} significant columns'.format(len(sigCols)))\n",
    "    print('The significant columns are: ')\n",
    "    print(sig_features[sig_features['P_Value']<.1])\n",
    "\n",
    "    sigCols = np.append(sigCols, 'interest_level')\n",
    "\n",
    "    #Create a simplified df with only the significant columns\n",
    "    validLogTest = test[~pd.isnull(test['prob_interest_building'])]\n",
    "    \n",
    "    simpleTrain = train[sigCols]\n",
    "    simpleTest = validLogTest[sigCols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(simpleTrain.drop('interest_level',axis=1),\n",
    "                                                        simpleTrain['interest_level'], \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=42)\n",
    "    \n",
    "    print('Running Logistic Regression on best data...')\n",
    "    print('Training Logistic Regression...')\n",
    "    logReg = logCV(cv=10)\n",
    "    logReg.fit(X_train, y_train)\n",
    "    print(classification_report(logReg.predict(X_test), y_test))\n",
    "    \n",
    "    logReg.fit(simpleTrain.drop('interest_level', axis=1), \n",
    "               simpleTrain['interest_level'])\n",
    "    \n",
    "    print(str(len(simpleTest)) + ' samples. Predicting Interest Levels...')\n",
    "    logPreds = logReg.predict(simpleTest.drop('interest_level', axis=1))\n",
    "\n",
    "    validLogTest['interest_level'] = logPreds\n",
    "    \n",
    "    print('Running SVM on lower quality data.')\n",
    "    \n",
    "    # Because some of the data is still unknown, \n",
    "    # we have to use an SVM to classify about 48% of the test data\n",
    "    \n",
    "    # Drop the columns that are objects, but keep the \n",
    "    # interest_level classification\n",
    "    svm_tr = train.drop(['interestVal','building_id','created',\n",
    "                         'description','display_address','features',\n",
    "                         'manager_id','photos','type','street_address',\n",
    "                         'prob_interest_manager', 'prob_interest_building', \n",
    "                         'prob_buildManager'],axis=1)\n",
    "    \n",
    "    # Break into training and test sets to train SVM model\n",
    "    X_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(svm_tr.drop(['interest_level'],axis=1),\n",
    "                                                                        svm_tr['interest_level'], test_size=0.2, \n",
    "                                                                        random_state=42)\n",
    "\n",
    "    scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_svm_train)\n",
    "\n",
    "    # Train SVM with C and gamma values calculated in hyperparameter_selection.ipynb\n",
    "    svm_model = SVC(cache_size=7000, \n",
    "                    decision_function_shape='multinominal',\n",
    "                    C=1.7782794100389228, \n",
    "                    gamma=9.9999999999999995e-07)\n",
    "    print('Training SVM on lower quality data...')\n",
    "    svm_model.fit(X_svm_train, y_svm_train)\n",
    "    print(classification_report(svm_model.predict(X_svm_test), y_svm_test))\n",
    "\n",
    "    # Drop features of type object from test set\n",
    "    svmTest = test[pd.isnull(test['prob_interest_building'])]\n",
    "    svmTestSimple = svmTest.select_dtypes(exclude=['object'])\n",
    "    svmTestSimple.drop(['interestVal', 'prob_buildManager', 'interest_level',  \n",
    "                        'prob_interest_building', 'prob_interest_manager'],\n",
    "                       axis=1, inplace=True)\n",
    "    svmTestSimple.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(str(len(svmTestSimple)) + ' samples. Fitting SVM of training...')\n",
    "    svm_model.fit(svm_tr.drop('interest_level', axis=1), \n",
    "                  svm_tr['interest_level'])\n",
    "    \n",
    "    # Predict unclassified data with SVM model\n",
    "    print('Predicting Interest Levels for Test data...')\n",
    "    svm_preds = svm_model.predict(svmTestSimple)\n",
    "    svmTest['interest_level'] = svm_preds\n",
    "\n",
    "    # Add classification results from SVM to classification results \n",
    "    # from log regression\n",
    "    test = pd.concat([validLogTest, svmTest])\n",
    "\n",
    "    print('Predictions complete.')\n",
    "    return test['interest_level'], test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning...\n",
      "Cleaning Complete. Processing descriptions to determine type...\n",
      "Typing Complete. Generating Interaction Terms...\n",
      "Cleaning and Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_json('./raw_data/train_data.json')\n",
    "test = pd.read_json('./raw_data/test_data.json')\n",
    "\n",
    "cleanedDF, cleanedTyped, train, test = cleanPreprocessData(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 7 significant columns\n",
      "The significant columns are: \n",
      "                         P_Value\n",
      "Common_Outdoor_Space    0.029706\n",
      "No_Fee                  0.000172\n",
      "bathrooms               0.027459\n",
      "prob_buildManager       0.000000\n",
      "prob_interest_building  0.000000\n",
      "prob_interest_manager   0.000000\n",
      "rooms                   0.001446\n",
      "Running Logistic Regression on best data...\n",
      "Training Logistic Regression...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.83      0.81      0.82       646\n",
      "        low       0.98      0.92      0.95      6226\n",
      "     medium       0.70      0.88      0.78      1512\n",
      "\n",
      "avg / total       0.92      0.90      0.91      8384\n",
      "\n",
      "3776samples. Predicting Interest Levels...\n",
      "Running SVM on lower quality data.\n",
      "Training SVM on lower quality data...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.10      0.65      0.17        93\n",
      "        low       0.98      0.72      0.83      8010\n",
      "     medium       0.07      0.48      0.12       281\n",
      "\n",
      "avg / total       0.94      0.71      0.80      8384\n",
      "\n",
      "3658 samples. Fitting SVM of training...\n",
      "Predicting Interest Levels for Test data...\n",
      "Predictions complete.\n"
     ]
    }
   ],
   "source": [
    "preds, test = runModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expected accuracy is $$\\frac{.71(3658) + .90 (3776)}{3658+3776} = 80.65\\%.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds.to_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('test_with_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
