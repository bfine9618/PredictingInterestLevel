{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV as logCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Data Visualization:\n",
    "https://github.com/bfine9618/ese305FinalProject/blob/master/figures/Initial%20Visualization.ipynb\n",
    "\n",
    "Initial Exploration of Various Models:\n",
    "\n",
    "   * Trees (Standard Tree, Random Forest Classifier, Bagged Trees):\n",
    "   https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/Trees.ipynb\n",
    "   \n",
    "   \n",
    "   * Linear Model (to get a sense of relevant predictors):\n",
    " https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/linear_model.ipynb\n",
    "    \n",
    "    \n",
    "Exploration of Features/Data Cleaning:\n",
    "\n",
    "   * Testing for Significant Interaction Terms:\n",
    "  https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/exploring_interaction_terms.ipynb\n",
    "   \n",
    "  \n",
    "   * Using NLP to classify housing listings by the type of home:\n",
    "   https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/NLP.ipynb\n",
    "   \n",
    "   \n",
    "   * Data Cleaning (describe more):\n",
    "   https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/Data%20Cleaning%20and%20Pre-Processing.ipynb\n",
    "   \n",
    "   \n",
    "   * Data Cleaning 2 (describe more): \n",
    " https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20cleaning/Merge%20Data%20Files.ipynb\n",
    "   \n",
    "\n",
    "Model 1: Fitting a Random Forest Classifier on the 'typed' housing listings (see NLP) and exploring fitting a series of 2-class Random Forest Classifiers to improve prediction:\n",
    "https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/Trees_Typed_Data.ipynb\n",
    "\n",
    "Model 2: SVMs\n",
    "\n",
    "   * SVM Initial Exploration:\n",
    " https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/SVM_Exploration.ipynb\n",
    "   \n",
    "   \n",
    "   * Hyperparameter Selection (of gamma and c): \n",
    "https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/hyperparameter_selection_SVM.ipynb\n",
    "   \n",
    "\n",
    "(Brief) Exploration of Neural Nets, with subpar success:\n",
    "https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/NeuralNets.ipynb\n",
    "\n",
    "Model 3: K-Nearest Neighbors and Logistic Regression on ____: \n",
    "https://github.com/bfine9618/ese305FinalProject/blob/master/Initial%20models/KNN%20%26%20CV%20Logistic%20Reg.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model:\n",
    "\n",
    "Describe process here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are our helper functions and classes. They each assist with a different part \n",
    "# of cleaning and preprocessing the data.\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def descrClean(x):\n",
    "    des = strip_tags(x)\n",
    "    return des.lower()\n",
    "\n",
    "# Function to Classify Unit Types\n",
    "def unitType(x, types):\n",
    "    homeType = {    \n",
    "        }\n",
    "    for lst in types:\n",
    "        homeType[lst[0]] = False\n",
    "    \n",
    "    for lst in types:\n",
    "        for w in lst:\n",
    "            if w in x:\n",
    "                return lst[0]\n",
    "    return 'other'\n",
    "\n",
    "#Dealing with lofts or studios that have no rooms \n",
    "def pricePerRoom(row):\n",
    "    if row['rooms']==0:\n",
    "        return row['price']/.5\n",
    "    else:\n",
    "        return row['price']/row['rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanPreprocessData(train, test):\n",
    "    print('Cleaning...')\n",
    "    df = train\n",
    "    test['test'] = True\n",
    "\n",
    "    #Merge the two files to clean and comput interaction terms.\n",
    "    df = df.append(test)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['test'].fillna(False, inplace=True)\n",
    "\n",
    "    #Clean the column names for regressions and ML Models\n",
    "    df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('-', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('/', '_') for c in df.columns]\n",
    "\n",
    "    #Confirm types for columns with numbers\n",
    "    df['bedrooms'] = df['bedrooms'].apply(float)\n",
    "    df['bedrooms'].fillna(0, inplace=True)\n",
    "    df['bathrooms'].fillna(0, inplace=True)\n",
    "\n",
    "    #Drop meaningless columns in data\n",
    "    df.drop(['index', 'level_0'], axis=1, inplace=True)\n",
    "\n",
    "    #Map Interest levels to values for OLS Regression\n",
    "    df['interestVal'] = df['interest_level'].map({'high': 1, 'medium': 0.5, 'low':0})\n",
    "\n",
    "    #Clean the HTML from descriptions to allow for NLP\n",
    "    df['description'] = df['description'].apply(descrClean)\n",
    "\n",
    "    # Aggregate to create one laundry in building column that isn't case sensitive\n",
    "    df['laundry_in_building'] = df.apply(lambda row: row['Laundry_in_Building'] or row['Laundry_In_Building'], axis=1)\n",
    "\n",
    "    # Drop old laundry in building columns\n",
    "    df = df.drop(['Laundry_in_Building', 'Laundry_In_Building'], axis=1)\n",
    "\n",
    "    cleanedDf = df\n",
    "\n",
    "    print('Cleaning Complete. Processing descriptions to determine type...')\n",
    "\n",
    "    # To determine the type of rental unit, we conduct a basic NLP\n",
    "    # Define basic unit types\n",
    "    apt = ['apartment', 'apt']\n",
    "    condo = ['condominium', 'condo']\n",
    "    walkUp = ['walk_up', 'walk-up', 'walkup', 'walk up']\n",
    "    studio = ['studio']\n",
    "    ph = ['ph', 'penhouse']\n",
    "    townhome = ['townhome', 'duplex', 'townhouse']\n",
    "    loft = ['loft']\n",
    "\n",
    "    types = [apt, condo, walkUp, studio, ph, townhome, loft]\n",
    "\n",
    "    #Determine rental type\n",
    "    df['type'] = df['description'].apply(lambda x : unitType(x, types)) \n",
    "\n",
    "    #Determine if a type has been found\n",
    "    df['foundType'] = ~df['type'].str.contains('other') \n",
    "\n",
    "    #Create binary dummy columns for each type\n",
    "    df = pd.concat([df, pd.get_dummies(df['type'])], axis=1) \n",
    "\n",
    "    #Combine and drop the two loft column\n",
    "    df['loft'].fillna(False, inplace=True)\n",
    "    df['loft'] = df[['loft', 'Loft']].apply(lambda row : row['loft'] or row['Loft'], axis=1)\n",
    "    df.drop('Loft', axis=1, inplace=True)\n",
    "\n",
    "    cleanedTyped = df\n",
    "\n",
    "    print('Typing Complete. Generating Interaction Terms...')\n",
    "\n",
    "    # Generate interaction terms to find differentiators\n",
    "    # Luxury Score Term - higher the score means the more luxury items included\n",
    "    df['lux_score'] = (df['Exclusive'] + df['Doorman'] + df['Outdoor_Space'] + \n",
    "                        df['New_Construction'] + df['Roof_Deck'] + df['Fitness_Center'] + \n",
    "                        df['Swimming_Pool'] + df['Elevator'] + df['Laundry_in_Unit'] + \n",
    "                        df['Hardwood_Floors']) / 10\n",
    "\n",
    "    #Group data by buildings and agents to determine expected interest -----MAGIC FEATURE-----\n",
    "    agentGroup = df.groupby(['manager_id']).mean()\n",
    "    buildingGroup = df.groupby(['building_id', 'manager_id']).mean()\n",
    "\n",
    "    buildingAvg = buildingGroup[['interestVal']]\n",
    "    buildingAvg.columns = ['prob_interest_building']\n",
    "    buildingAvg.reset_index(inplace=True)\n",
    "\n",
    "    managerAvg = agentGroup[['interestVal']]\n",
    "    managerAvg.columns = ['prob_interest_manager']\n",
    "    managerAvg.reset_index(inplace=True)\n",
    "\n",
    "    #Merge back to original DF\n",
    "    df = df.merge(managerAvg, on='manager_id', how='left')\n",
    "    df = df.merge(buildingAvg, on=['building_id', 'manager_id'], how='left')\n",
    "\n",
    "    #Compute expected interest by building and manager\n",
    "    df['prob_buildManager'] = (df['prob_interest_building']+df['prob_interest_manager'])/2\n",
    "\n",
    "    #Count rooms and determine price per room\n",
    "    df['rooms'] = df['bedrooms']+df['bathrooms']\n",
    "    \n",
    "    df['price_per_room'] = df[['price', 'rooms']].apply(pricePerRoom, axis=1)\n",
    "\n",
    "    # Number of Luxury Features Term\n",
    "    df['num_luxury'] = (df['Exclusive'] + df['Doorman'] + df['Outdoor_Space'] + df['New_Construction'] + df['Roof_Deck'] + df['Fitness_Center'] + df['Swimming_Pool'] + df['Elevator'] + df['Laundry_in_Unit'] + df['Hardwood_Floors'])\n",
    "\n",
    "    # Number of Features per Listing\n",
    "    df['num_features'] = df['features'].apply(len)\n",
    "\n",
    "    # ADA compatible interaction term\n",
    "    # 1 if both elevator and wheelchair access, 0 if one or neither are included\n",
    "    df['ada'] = df['Elevator'] * df['Wheelchair_Access']\n",
    "\n",
    "    # Create transformed term that creates a score for outdoor spaces\n",
    "    # Higher the score, the more of these features are included\n",
    "    df['outdoor_score'] = (df['Outdoor_Space'] + df['Balcony'] + df['Common_Outdoor_Space'] \n",
    "                           + df['Garden_Patio'] + df['Roof_Deck'] + df['Terrace']) / 6\n",
    "\n",
    "    # Create interaction term for fitness oriented\n",
    "    # 1 if both swimming pool and fitness center are included, 0 if one or neither included\n",
    "    df['fitness_oriented'] = df['Fitness_Center'] * df['Swimming_Pool']\n",
    "\n",
    "    # Create interaction term for doorman/exclusive\n",
    "    # 1 if both are included, 0 if one or neither are included\n",
    "    df['door_excl'] = df['Doorman'] * df['Exclusive']\n",
    "\n",
    "    # Create interaction term for cats and dogs allowed\n",
    "    # 1 if both are allowed, 0 if one or neither are allowed\n",
    "    df['pets_allowed'] = df['Cats_Allowed'] * df['Dogs_Allowed']\n",
    "\n",
    "    #Compute price per feature and price per luxury feature. \n",
    "    #If no features exist, the value is empty\n",
    "    df['price_per_feature'] = df['price']/df['num_features']\n",
    "    df['price_per_feature'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    df['price_per_num_lux'] = df['price']/df['num_luxury']\n",
    "    df['price_per_num_lux'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    #Determine expected prices by type of unit\n",
    "    g1 = df.groupby(['type']).mean()\n",
    "    g1.reset_index(inplace=True)\n",
    "\n",
    "    #Columns we wish to average\n",
    "    avgs = g1[['type','lux_score', 'num_features', \n",
    "               'num_luxury','outdoor_score', 'price_per_num_lux', \n",
    "               'price_per_feature']]\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    #Rename columns and merge back to original DF\n",
    "    avgs.columns = ['avg_'+x for x in avgs]\n",
    "    avgs.rename(columns={'avg_type':'type'}, inplace=True)\n",
    "    df = pd.merge(df, avgs, on='type')\n",
    "\n",
    "    #If no price was found, set the price for the column as average to avoid skewing the data\n",
    "    df['price_per_num_lux'].fillna(df['avg_price_per_num_lux'], inplace=True)\n",
    "    df['outdoor_score'].fillna(df['avg_outdoor_score'], inplace=True)\n",
    "    df['lux_score'].fillna(df['avg_lux_score'], inplace=True)\n",
    "    df['price_per_feature'].fillna(df['avg_price_per_feature'], inplace=True)\n",
    "\n",
    "    df['price_lux_ratio'] = df['price_per_num_lux']/df['avg_price_per_num_lux']\n",
    "    df['outdoor_ratio'] = df['outdoor_score']/df['avg_outdoor_score']\n",
    "    df['lux_ratio'] = df['lux_score']/df['avg_lux_score']\n",
    "    df['price_feature_ratio'] = df['price_per_feature']/df['avg_price_per_feature']\n",
    "\n",
    "\n",
    "    #Compute the number of photos included in the listing\n",
    "    df['numPhotos'] = df['photos'].apply(len)\n",
    "\n",
    "    #Listing id is an arbitrary int label assined to each listing. not useful for classification\n",
    "    df.drop(['listing_id'], axis=1, inplace=True)\n",
    "\n",
    "    #Output new training and testing datasets\n",
    "    train = pd.DataFrame(df[df['test']==False].dropna())\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    train.drop('test', inplace=True, axis=1)\n",
    "    \n",
    "    test = pd.DataFrame(df[df['test']])\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    test.drop('test', inplace=True, axis=1)\n",
    "\n",
    "    train.to_json('./cleaned/train.json')\n",
    "    test.to_json('./cleaned/test.json')\n",
    "    print('Cleaning and Preprocessing complete.')\n",
    "    return cleanedDf, cleanedTyped, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModel():\n",
    "    \n",
    "    train = pd.read_json('./cleaned/train.json')\n",
    "    test = pd.read_json('./cleaned/test.json')\n",
    "    \n",
    "    #Determine the columns with which to run an OLS, exclude the indicator column\n",
    "    data = train.drop('interestVal', axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    #join columns to build to equation\n",
    "    equation = ('+').join(data.columns)\n",
    "\n",
    "    #run the OLS to determine significant columns\n",
    "    model = smf.ols('interestVal~'+equation, data=train).fit()\n",
    "\n",
    "    #make a DF of significant features\n",
    "    sig_features = pd.DataFrame(model.pvalues, index=data.columns, columns={'P_Value'})\n",
    "\n",
    "    sigCols = sig_features[sig_features['P_Value']<.1].index.values\n",
    "    print('The data has {} significant columns'.format(len(sigCols)))\n",
    "    print('The significant columns are: ')\n",
    "    print(sig_features[sig_features['P_Value']<.1])\n",
    "\n",
    "    sigCols = np.append(sigCols, 'interest_level')\n",
    "\n",
    "    #Create a simplified df with only the significant columns\n",
    "    validLogTest = test[~pd.isnull(test['prob_interest_building'])]\n",
    "    \n",
    "    simpleTrain = train[sigCols]\n",
    "    simpleTest = validLogTest[sigCols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(simpleTrain.drop('interest_level',axis=1),\n",
    "                                                    simpleTrain['interest_level'], test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "    \n",
    "    print('Running Logistic Regression on best data...')\n",
    "    print('Training Logistic Regression...')\n",
    "    logReg = logCV(cv=10)\n",
    "    logReg.fit(X_train, y_train)\n",
    "    print(classification_report(logReg.predict(X_test), y_test))\n",
    "    \n",
    "    logReg.fit(simpleTrain.drop('interest_level', axis=1), simpleTrain['interest_level'])\n",
    "    \n",
    "    print(str(len(simpleTest)) + ' samples. Predicting Interest Levels...')\n",
    "    logPreds = logReg.predict(simpleTest.drop('interest_level', axis=1))\n",
    "\n",
    "    validLogTest['interest_level'] = logPreds\n",
    "    \n",
    "    print('Running SVM on lower quality data.')\n",
    "    \n",
    "    # Because some of the data is still unknown, we have to use an SVM to classify about 48% of the test data\n",
    "    \n",
    "    # Drop the columns that are objects, but keep the interest_level classification\n",
    "    svm_tr = train.drop(['interestVal','building_id','created','description','display_address','features',\n",
    "                        'manager_id','photos','type','street_address',\n",
    "                        'prob_interest_manager', 'prob_interest_building', 'prob_buildManager'],axis=1)\n",
    "    \n",
    "    # Break into training and test sets to train SVM model\n",
    "    X_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(svm_tr.drop(['interest_level'],axis=1),\n",
    "                                                                        svm_tr['interest_level'], test_size=0.2, \n",
    "                                                                        random_state=42)\n",
    "\n",
    "    scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_svm_train)\n",
    "\n",
    "    # Train SVM with C and gamma values calculated in hyperparameter_selection.ipynb\n",
    "    svm_model = SVC(cache_size=7000, decision_function_shape='multinominal',\n",
    "                    C=1.7782794100389228, gamma=9.9999999999999995e-07)\n",
    "    print('Training SVM on lower quality data...')\n",
    "    svm_model.fit(X_svm_train, y_svm_train)\n",
    "    print(classification_report(svm_model.predict(X_svm_test), y_svm_test))\n",
    "\n",
    "    # Drop features of type object from test set\n",
    "    svmTest = test[pd.isnull(test['prob_interest_building'])]\n",
    "    svmTestSimple = svmTest.select_dtypes(exclude=['object'])\n",
    "    svmTestSimple.drop(['interestVal', 'prob_buildManager', 'interest_level',  \n",
    "                        'prob_interest_building', 'prob_interest_manager'],axis=1, inplace=True)\n",
    "    svmTestSimple.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(str(len(svmTestSimple)) + ' samples. Fitting SVM of training...')\n",
    "    svm_model.fit(svm_tr.drop('interest_level', axis=1), svm_tr['interest_level'])\n",
    "    \n",
    "    # Predict unclassified data with SVM model\n",
    "    print('Predicting Interest Levels for Test data...')\n",
    "    svm_preds = svm_model.predict(svmTestSimple)\n",
    "    svmTest['interest_level'] = svm_preds\n",
    "\n",
    "    # Add classification results from SVM to classification results from log regression\n",
    "    test = pd.concat([validLogTest, svmTest])\n",
    "\n",
    "    print('Predictions complete.')\n",
    "    return test['interest_level'], test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning...\n",
      "Cleaning Complete. Processing descriptions to determine type...\n",
      "Typing Complete. Generating Interaction Terms...\n",
      "Cleaning and Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_json('./raw_data/train_data.json')\n",
    "test = pd.read_json('./raw_data/test_data.json')\n",
    "\n",
    "cleanedDF, cleanedTyped, train, test = cleanPreprocessData(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 7 significant columns\n",
      "The significant columns are: \n",
      "                         P_Value\n",
      "Common_Outdoor_Space    0.029706\n",
      "No_Fee                  0.000172\n",
      "bathrooms               0.027459\n",
      "prob_buildManager       0.000000\n",
      "prob_interest_building  0.000000\n",
      "prob_interest_manager   0.000000\n",
      "rooms                   0.001446\n",
      "Running Logistic Regression on best data...\n",
      "Training Logistic Regression...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.83      0.81      0.82       646\n",
      "        low       0.98      0.92      0.95      6226\n",
      "     medium       0.70      0.88      0.78      1512\n",
      "\n",
      "avg / total       0.92      0.90      0.91      8384\n",
      "\n",
      "3776samples. Predicting Interest Levels...\n",
      "Running SVM on lower quality data.\n",
      "Training SVM on lower quality data...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.10      0.65      0.17        93\n",
      "        low       0.98      0.72      0.83      8010\n",
      "     medium       0.07      0.48      0.12       281\n",
      "\n",
      "avg / total       0.94      0.71      0.80      8384\n",
      "\n",
      "3658 samples. Fitting SVM of training...\n",
      "Predicting Interest Levels for Test data...\n",
      "Predictions complete.\n"
     ]
    }
   ],
   "source": [
    "preds, test = runModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expected accuracy is (.71*3658+.90*3776)/(3658+3776) = 80.65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds.to_csv('test_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('test_with_preds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
