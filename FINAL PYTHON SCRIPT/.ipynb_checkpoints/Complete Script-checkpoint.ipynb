{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV as logCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def descrClean(x):\n",
    "    des = strip_tags(x)\n",
    "    return des.lower()\n",
    "\n",
    "# Function to Classify Unit Types\n",
    "def unitType(x, types):\n",
    "    homeType = {    \n",
    "        }\n",
    "    for lst in types:\n",
    "        homeType[lst[0]] = False\n",
    "    \n",
    "    for lst in types:\n",
    "        for w in lst:\n",
    "            if w in x:\n",
    "                return lst[0]\n",
    "    return 'other'\n",
    "\n",
    "\n",
    "def cleanPreprocessData(train, test):\n",
    "    print('Cleaning...')\n",
    "    df = train\n",
    "    test['test'] = True\n",
    "\n",
    "    #Merge the two files to clean and comput interaction terms.\n",
    "    df = df.append(test)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['test'].fillna(False, inplace=True)\n",
    "\n",
    "    #Clean the column names for regressions and ML Models\n",
    "    df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('-', '_') for c in df.columns]\n",
    "    df.columns = [c.replace('/', '_') for c in df.columns]\n",
    "\n",
    "    #Confirm types for columns with numbers\n",
    "    df['bedrooms'] = df['bedrooms'].apply(float)\n",
    "    df['bedrooms'].fillna(0, inplace=True)\n",
    "    df['bathrooms'].fillna(0, inplace=True)\n",
    "\n",
    "    #Drop meaningless columns in data\n",
    "    df.drop(['index', 'level_0'], axis=1, inplace=True)\n",
    "\n",
    "    #Map Interest levels to values for OLS Regression\n",
    "    df['interestVal'] = df['interest_level'].map({'high': 1, 'medium': 0.5, 'low':0})\n",
    "\n",
    "    #Clean the HTML from descriptions to allow for NLP\n",
    "    df['description'] = df['description'].apply(descrClean)\n",
    "\n",
    "    # Aggregate to create one laundry in building column that isn't case sensitive\n",
    "    df['laundry_in_building'] = df.apply(lambda row: row['Laundry_in_Building'] or row['Laundry_In_Building'], axis=1)\n",
    "\n",
    "    # Drop old laundry in building columns\n",
    "    df = df.drop(['Laundry_in_Building', 'Laundry_In_Building'], axis=1)\n",
    "\n",
    "    cleanedDf = df\n",
    "\n",
    "    print('Cleaning Complete. Processing descriptions to determine type...')\n",
    "\n",
    "    # To determine the type of rental unit, we conduct a basic NLP\n",
    "    # Define basic unit types\n",
    "    apt = ['apartment', 'apt']\n",
    "    condo = ['condominium', 'condo']\n",
    "    walkUp = ['walk_up', 'walk-up', 'walkup', 'walk up']\n",
    "    studio = ['studio']\n",
    "    ph = ['ph', 'penhouse']\n",
    "    townhome = ['townhome', 'duplex', 'townhouse']\n",
    "    loft = ['loft']\n",
    "\n",
    "    types = [apt, condo, walkUp, studio, ph, townhome, loft]\n",
    "\n",
    "    #Determine rental type\n",
    "    df['type'] = df['description'].apply(lambda x : unitType(x, types)) \n",
    "\n",
    "    #Determine if a type has been found\n",
    "    df['foundType'] = ~df['type'].str.contains('other') \n",
    "\n",
    "    #Create binary dummy columns for each type\n",
    "    df = pd.concat([df, pd.get_dummies(df['type'])], axis=1) \n",
    "\n",
    "    #Combine and drop the two loft column\n",
    "    df['loft'].fillna(False, inplace=True)\n",
    "    df['loft'] = df[['loft', 'Loft']].apply(lambda row : row['loft'] or row['Loft'], axis=1)\n",
    "    df.drop('Loft', axis=1, inplace=True)\n",
    "\n",
    "    cleanedTyped = df\n",
    "\n",
    "    print('Typing Complete. Generating Interaction Terms...')\n",
    "\n",
    "    # Generate interaction terms to find differentiators\n",
    "    # Luxury Score Term - higher the score means the more luxury items included\n",
    "    df['lux_score'] = (df['Exclusive'] + df['Doorman'] + df['Outdoor_Space'] + \n",
    "                        df['New_Construction'] + df['Roof_Deck'] + df['Fitness_Center'] + \n",
    "                        df['Swimming_Pool'] + df['Elevator'] + df['Laundry_in_Unit'] + \n",
    "                        df['Hardwood_Floors']) / 10\n",
    "\n",
    "    #Group data by buildings and agents to determine expected interest -----MAGIC FEATURE-----\n",
    "    agentGroup = df.groupby(['manager_id']).mean()\n",
    "    buildingGroup = df.groupby(['building_id', 'manager_id']).mean()\n",
    "\n",
    "    buildingAvg = buildingGroup[['interestVal']]\n",
    "    buildingAvg.columns = ['prob_interest_building']\n",
    "    buildingAvg.reset_index(inplace=True)\n",
    "\n",
    "    managerAvg = agentGroup[['interestVal']]\n",
    "    managerAvg.columns = ['prob_interest_manager']\n",
    "    managerAvg.reset_index(inplace=True)\n",
    "\n",
    "    #Merge back to original DF\n",
    "    df = df.merge(managerAvg, on='manager_id', how='left')\n",
    "    df = df.merge(buildingAvg, on=['building_id', 'manager_id'], how='left')\n",
    "\n",
    "    #Compute expected interest by building and manager\n",
    "    df['prob_buildManager'] = (df['prob_interest_building']+df['prob_interest_manager'])/2\n",
    "\n",
    "    #Count rooms and determine price per room\n",
    "    df['rooms'] = df['bedrooms']+df['bathrooms']\n",
    "    df['price_per_room'] = df['price']/df['rooms']\n",
    "\n",
    "    # Number of Luxury Features Term\n",
    "    df['num_luxury'] = (df['Exclusive'] + df['Doorman'] + df['Outdoor_Space'] + df['New_Construction'] + df['Roof_Deck'] + df['Fitness_Center'] + df['Swimming_Pool'] + df['Elevator'] + df['Laundry_in_Unit'] + df['Hardwood_Floors'])\n",
    "\n",
    "    # Number of Features per Listing\n",
    "    df['num_features'] = df['features'].apply(len)\n",
    "\n",
    "    # ADA compatible interaction term\n",
    "    # 1 if both elevator and wheelchair access, 0 if one or neither are included\n",
    "    df['ada'] = df['Elevator'] * df['Wheelchair_Access']\n",
    "\n",
    "    # Create transformed term that creates a score for outdoor spaces\n",
    "    # Higher the score, the more of these features are included\n",
    "    df['outdoor_score'] = (df['Outdoor_Space'] + df['Balcony'] + df['Common_Outdoor_Space'] \n",
    "                           + df['Garden_Patio'] + df['Roof_Deck'] + df['Terrace']) / 6\n",
    "\n",
    "    # Create interaction term for fitness oriented\n",
    "    # 1 if both swimming pool and fitness center are included, 0 if one or neither included\n",
    "    df['fitness_oriented'] = df['Fitness_Center'] * df['Swimming_Pool']\n",
    "\n",
    "    # Create interaction term for doorman/exclusive\n",
    "    # 1 if both are included, 0 if one or neither are included\n",
    "    df['door_excl'] = df['Doorman'] * df['Exclusive']\n",
    "\n",
    "    # Create interaction term for cats and dogs allowed\n",
    "    # 1 if both are allowed, 0 if one or neither are allowed\n",
    "    df['pets_allowed'] = df['Cats_Allowed'] * df['Dogs_Allowed']\n",
    "\n",
    "    #Compute price per feature and price per luxury feature. \n",
    "    #If no features exist, the value is empty\n",
    "    df['price_per_feature'] = df['price']/df['num_features']\n",
    "    df['price_per_feature'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    df['price_per_num_lux'] = df['price']/df['num_luxury']\n",
    "    df['price_per_num_lux'].replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "    #Determine expected prices by type of unit\n",
    "    g1 = df.groupby(['type']).mean()\n",
    "    g1.reset_index(inplace=True)\n",
    "\n",
    "    #Columns we wish to average\n",
    "    avgs = g1[['type','lux_score', 'num_features', \n",
    "               'num_luxury','outdoor_score', 'price_per_num_lux', \n",
    "               'price_per_feature']]\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    #Rename columns and merge back to original DF\n",
    "    avgs.columns = ['avg_'+x for x in avgs]\n",
    "    avgs.rename(columns={'avg_type':'type'}, inplace=True)\n",
    "    df = pd.merge(df, avgs, on='type')\n",
    "\n",
    "    #If no price was found, set the price for the column as average to avoid skewing the data\n",
    "    df['price_per_num_lux'].fillna(df['avg_price_per_num_lux'], inplace=True)\n",
    "    df['outdoor_score'].fillna(df['avg_outdoor_score'], inplace=True)\n",
    "    df['lux_score'].fillna(df['avg_lux_score'], inplace=True)\n",
    "    df['price_per_feature'].fillna(df['avg_price_per_feature'], inplace=True)\n",
    "\n",
    "    df['price_lux_ratio'] = df['price_per_num_lux']/df['avg_price_per_num_lux']\n",
    "    df['outdoor_ratio'] = df['outdoor_score']/df['avg_outdoor_score']\n",
    "    df['lux_ratio'] = df['lux_score']/df['avg_lux_score']\n",
    "    df['price_feature_ratio'] = df['price_per_feature']/df['avg_price_per_feature']\n",
    "\n",
    "\n",
    "    #Compute the number of photos included in the listing\n",
    "    df['numPhotos'] = df['photos'].apply(len)\n",
    "\n",
    "    #Listing id is an arbitrary int label assined to each listing. not useful for classification\n",
    "    df.drop(['listing_id'], axis=1, inplace=True)\n",
    "\n",
    "    #Output new training and testing datasets\n",
    "    train = pd.DataFrame(df[df['test']==False].dropna())\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    train.drop('test', inplace=True, axis=1)\n",
    "    \n",
    "    test = pd.DataFrame(df[df['test']])\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    test.drop('test', inplace=True, axis=1)\n",
    "\n",
    "    train.to_json('./cleaned/train.json')\n",
    "    test.to_json('./cleaned/test.json')\n",
    "\n",
    "    return cleanedDf, cleanedTyped, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel():\n",
    "    \n",
    "    train = pd.read_json('./cleaned/train.json')\n",
    "    test = pd.read_json('./cleaned/test.json')\n",
    "    \n",
    "    #Determine the columns with which to run an OLS, exclude the indicator column\n",
    "    data = train.drop('interestVal', axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    #join columns to build to equation\n",
    "    equation = ('+').join(data.columns)\n",
    "\n",
    "    #run the OLS to determine significant columns\n",
    "    model = smf.ols('interestVal~'+equation, data=train).fit()\n",
    "\n",
    "    #make a DF of significant features\n",
    "    sig_features = pd.DataFrame(model.pvalues, index=data.columns, columns={'P_Value'})\n",
    "\n",
    "    sigCols = sig_features[sig_features['P_Value']<.1].index.values\n",
    "    print('The data has {} significant columns'.format(len(sigCols)))\n",
    "    print('The significant columns are: ')\n",
    "    print(sig_features[sig_features['P_Value']<.1])\n",
    "\n",
    "    sigCols = np.append(sigCols, 'interest_level')\n",
    "\n",
    "    #Create a simplified df with only the significant columns\n",
    "    validLogTest = test[~pd.isnull(test['prob_interest_building'])]\n",
    "    \n",
    "    simpleTrain = train[sigCols]\n",
    "    simpleTest = validLogTest[sigCols]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(simpleTrain.drop('interest_level',axis=1),\n",
    "                                                    simpleTrain['interest_level'], test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "    \n",
    "    print('Running Logistic Regression on best data...')\n",
    "    \n",
    "    logReg = logCV(cv=10)\n",
    "    logReg.fit(X_train, y_train)\n",
    "    print(classification_report(logReg.predict(X_test), y_test))\n",
    "\n",
    "    logReg.fit(simpleTrain.drop('interest_level', axis=1), simpleTrain['interest_level'])\n",
    "    logPreds = logReg.predict(simpleTest.drop('interest_level', axis=1))\n",
    "\n",
    "    validLogTest['interest_level'] = logPreds\n",
    "    \n",
    "    print('Running SVM on lower quality data...')\n",
    "    \n",
    "    # Because some of the data is still unknown, we have to use an SVM to classify about 48% of the test data\n",
    "    svmTest = test[pd.isnull(test['prob_interest_building'])]\n",
    "    svmTest = svmTest.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    svm_tr = train.copy().drop(['interestVal','building_id','created','description','display_address','features',\n",
    "                          'manager_id','photos','type','street_address'],axis=1)\n",
    "    X_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(svm_tr.drop(['interest_level'],axis=1),\n",
    "                                                                        svm_tr['interest_level'], test_size=0.33, \n",
    "                                                                        random_state=42)\n",
    "    svm_model = SVC()\n",
    "    print(X_svm_train.columns)\n",
    "#     print(X_svm_train['price_feature_ratio'].isnull().value_counts())\n",
    "#     print(X_svm_train['price_lux_ratio'].isnull().value_counts())\n",
    "#     print(X_svm_train['price_per_num_lux'].isnull().value_counts())\n",
    "#     print(X_svm_train['price_per_feature'].isnull().value_counts())\n",
    "    print(X_svm_train['price_per_room'].isnull().value_counts())\n",
    "    X_svm_train = X_svm_train.replace(np.inf, np.nan).fillna(X_svm_train['price'] / 2)\n",
    "    print(X_svm_train['price_per_room'].isnull().value_counts())\n",
    "    \n",
    "    svm_model.fit(X_svm_train, y_svm_train)\n",
    "    print(classification_report(svm_model.predict(X_svm_test), y_svm_test))\n",
    "    svm_preds = svm_model.predict(svmTest)\n",
    "    validLogTest['interest_level'] = svm_preds\n",
    "    \n",
    "    \n",
    "    return validLogTest['interest_level'], test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning...\n",
      "Cleaning Complete. Processing descriptions to determine type...\n",
      "Typing Complete. Generating Interaction Terms...\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_json('./raw_data/train_data.json')\n",
    "test = pd.read_json('./raw_data/test_data.json')\n",
    "\n",
    "cleanedDF, cleanedTyped, train, test = cleanPreprocessData(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 7 significant columns\n",
      "The significant columns are: \n",
      "                         P_Value\n",
      "Common_Outdoor_Space    0.032009\n",
      "No_Fee                  0.000164\n",
      "bathrooms               0.025488\n",
      "prob_buildManager       0.000000\n",
      "prob_interest_building  0.000000\n",
      "prob_interest_manager   0.000000\n",
      "rooms                   0.008104\n",
      "Running Logistic Regression on best data...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       high       0.83      0.80      0.81      1057\n",
      "        low       0.98      0.92      0.95     10311\n",
      "     medium       0.70      0.89      0.78      2465\n",
      "\n",
      "avg / total       0.92      0.91      0.91     13833\n",
      "\n",
      "Running SVM on lower quality data...\n",
      "Index(['Balcony', 'Cats_Allowed', 'Common_Outdoor_Space', 'Dining_Room',\n",
      "       'Dishwasher', 'Dogs_Allowed', 'Doorman', 'Elevator', 'Exclusive',\n",
      "       'Fitness_Center', 'Garden_Patio', 'Hardwood_Floors',\n",
      "       'High_Speed_Internet', 'Laundry_in_Unit', 'New_Construction', 'No_Fee',\n",
      "       'Outdoor_Space', 'Pre_War', 'Roof_Deck', 'Swimming_Pool', 'Terrace',\n",
      "       'Wheelchair_Access', 'ada', 'apartment', 'avg_lux_score',\n",
      "       'avg_num_features', 'avg_num_luxury', 'avg_outdoor_score',\n",
      "       'avg_price_per_feature', 'avg_price_per_num_lux', 'bathrooms',\n",
      "       'bedrooms', 'condominium', 'door_excl', 'fitness_oriented', 'foundType',\n",
      "       'latitude', 'laundry_in_building', 'loft', 'longitude', 'lux_ratio',\n",
      "       'lux_score', 'numPhotos', 'num_features', 'num_luxury', 'other',\n",
      "       'outdoor_ratio', 'outdoor_score', 'pets_allowed', 'ph', 'price',\n",
      "       'price_feature_ratio', 'price_lux_ratio', 'price_per_feature',\n",
      "       'price_per_num_lux', 'price_per_room', 'prob_buildManager',\n",
      "       'prob_interest_building', 'prob_interest_manager', 'rooms', 'studio',\n",
      "       'townhome', 'walk_up'],\n",
      "      dtype='object')\n",
      "False    27992\n",
      "True        93\n",
      "Name: price_per_room, dtype: int64\n",
      "False    27992\n",
      "True        93\n",
      "Name: price_per_room, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-c595797da12e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-f2c9122f5849>\u001b[0m in \u001b[0;36mrunModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_svm_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price_per_room'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0msvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_svm_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_svm_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_svm_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_svm_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0msvm_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvmTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BrookeB\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BrookeB\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Users\\BrookeB\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BrookeB\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "preds, test = runModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
